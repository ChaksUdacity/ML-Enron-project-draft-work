# Create a pipeline that extracts features from the data then creates a model

print "Pipeline 2: Feature Extraction and Modeling"
#Feature extraction is another procedure that is susceptible to data leakage.

#Like data preparation, feature extraction procedures must be restricted to the data in your training dataset.

#The pipeline provides a handy tool called the FeatureUnion which allows the results of multiple feature selection and extraction procedures to be combined into a larger dataset on which a model can be trained. Importantly, all the feature extraction and the feature union occurs within each fold of the cross validation procedure.

#The example below demonstrates the pipeline defined with four steps:

#Feature Extraction with Principal Component Analysis (3 features)
#Feature Extraction with Statistical Selection (6 features)
#Feature Union
#Learn a Logistic Regression Model
#The pipeline is then evaluated using 10-fold cross validation.

"""
Pipeline: chaining estimators

Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves two purposes here:
Convenience: You only have to call fit and predict once on your data to fit a whole sequence of estimators.
Joint parameter selection: You can grid search over parameters of all estimators in the pipeline at once.
All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.).
"""
#!/usr/bin/python

import sys
import pickle
sys.path.append("../tools/")
import pandas



from pandas import read_csv
from sklearn.cross_validation import KFold
from sklearn.cross_validation import cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.pipeline import FeatureUnion
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest

from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

from sklearn import cross_validation
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

with open("final_project_dataset.pkl", "r") as data_file:
    data_dict = pickle.load(data_file)
    
# Removing Obvious Outliners from Data_dict
    data_dict.pop('TOTAL',0)  
    data_dict.pop("THE TRAVEL AGENCY IN THE PARK", 0)

# DATA -- > Pandas DataFrame; 
import pandas as pd
df = pd.DataFrame.from_records(list(data_dict.values()))
employees = pd.Series(list(data_dict.keys())) 
df.set_index(employees, inplace=True) # Index is employees name
df_dict = df.to_dict('index')

# Drop features that have more than 80 missing values 
df.drop(['loan_advances',
         'director_fees',
         'restricted_stock_deferred',
         'deferral_payments',
         'deferred_income',
         'long_term_incentive',
         'email_address'],axis=1,inplace=True)


# replace NaN by 0, and all should be float
df.replace(to_replace= 'NaN', value= 0,inplace=True)
df=df.astype(float)



outlier_people =['LAVORATO JOHN J','KAMINSKI WINCENTY J',
                 'FREVERT MARK A','WHITE JR THOMAS E'] 

# Drop those people, outliers
df.drop(['LAVORATO JOHN J','KAMINSKI WINCENTY J',
         'FREVERT MARK A','WHITE JR THOMAS E',
        'PAI LOU L','BHATNAGAR SANJAY'] ,axis=0, inplace=True)


# reshape the numpy arrays:
from_poi = df.from_poi_to_this_person.values.reshape(-1, 1)
to_poi =df.from_this_person_to_poi.values.reshape(-1, 1)
to_messages=df.to_messages.values.reshape(-1, 1)
from_messages=df.from_messages.values.reshape(-1, 1)

# Feature Scaling 
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

n_from_poi =scaler.fit_transform(from_poi)
n_to_poi=scaler.fit_transform(to_poi)

n_to_messages=scaler.fit_transform(to_messages)
n_from_messages=scaler.fit_transform(from_messages)

# Creating new feature poi emails 
poi_emails = n_from_poi + n_to_poi

# Create lists from the results:
poi_emails = [n[0] for n in list(poi_emails)]

poi_emails = pd.Series(poi_emails)

# Creating new feature all emails
all_emails = n_to_messages + n_from_messages

# Create lists from the results:
all_emails = [n[0] for n in list(all_emails)]

all_emails = pd.Series(all_emails)

#Creating new feature the ratio poi emails in all emails
poi_email_ratio = list((poi_emails+1)/(all_emails+1))

# Add new feature poi email ratio to the dataset
df['poi_email_ratio'] = pd.Series(poi_email_ratio, index=df.index)


#  Peek at the Data before adding and deleting data
print " Top rows of data before adding and deleting data columns"
# head
print df.head()


df['new_poi'] = df['poi']

# for convenience, we drop the original feature
df.drop('poi', axis=1, inplace=True)



#  Peek at the Data after adding and deleting data
print " Top rows of data after before adding and deleting data columns"
# head
print df.head()
#print df.info

# Transform that dataframe into a dictionary so that you can use it for the rest of your code:


### features_list is a list of strings, each of which is a feature name
### first feature must be "poi", as this will be singled out as the label

new_features_list = df.columns.values
# add a print statement to see the features list
# check that `'poi'` is the first item in that list (for later):
print new_features_list


# 2. The dictionary, derived from that dataframe is given by:

df_dict = df.to_dict('index')

# The above two lines, after we have finished with the dataframe, give a dictionary that is compatible with the rest of the code in 'poi_id.py'.

#That is, we use that dictionary, 'df_dict', instead of reloading the original dictionary

# my_dataset will be the dictionary generated from pandas:
my_dataset = df_dict

#from feature_format import featureFormat
#from feature_format import targetFeatureSplit

#these two lines extract the features specified in features_list
# and extract them from data_dict, returning a numpy array
#data = featureFormat(df_dict, new_features_list)

# if you are creating new features, could also do that here
#labels, features = targetFeatureSplit(data)

#Y = labels
#X = features

array = df.values
X = array[:,0:14]
#X = features
Y = array[:,14]
#validation_size = 0.20
#seed = 7

from sklearn import preprocessing
# create feature union



#A pipeline is simply an instruction to sequentially apply processing (such as scaling), feature selection (such as SelectKBest), and classification (such as GaussianNB).

#When a pipeline is 'fit' then each of the steps in the pipeline are fit in sequence.

#So, for example, you want to apply scaling:

scaler = preprocessing.MinMaxScaler()
#to add that to a pipeline with 'SelectKBest' and 'GaussianNB' (as an example), you would use:

#to add that to a pipeline with 'SelectKBest' and 'GaussianNB' (as an example), you would use:

skb = SelectKBest(k = 14)
from sklearn.naive_bayes import GaussianNB
clf =  Pipeline(steps=[('scaling',scaler),("SKB", skb), ("NaiveBayes", GaussianNB())])
#then you can use the standard methods, such as:

validation_size = 0.20
seed = 7


X_train, X_validation, Y_train, Y_validation = cross_validation.train_test_split(X, Y, test_size=validation_size, random_state=seed)


clf.fit(X_train, Y_train)
predictions = clf.predict(X_validation)
print "Predictions using Pipeline:\n", predictions

print "accuracy score:\n", (accuracy_score(Y_validation, predictions))
print "confusion_matrix:\n",(confusion_matrix(Y_validation, predictions))
print "classification_report:\n", (classification_report(Y_validation, predictions))



# AlTERNATE METHOD USING GRIDSEARCH
print "# AlTERNATE METHOD USING GRIDSEARCH"
from sklearn.datasets import load_digits
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.decomposition import PCA, NMF
from sklearn.feature_selection import SelectKBest, chi2

#Currently you have code that fits a pipeline, and code that runs GridSearchCV. The next step is to combine the two:
#For GridSearchCV, the names that you use in the parameter grid are derived from the names of the methods in the pipeline, followed by two underscores.

#So, for example, you can extend your example here:

skb = SelectKBest(k = 14)
g_clf =  Pipeline(steps=[('scaling',scaler),("SKB", skb), ("NaiveBayes", GaussianNB())])
g_clf.fit(X_train, Y_train)


#using the same pipeline:
pipe =  Pipeline(steps=[('scaling',scaler),("SKB", skb), ("NaiveBayes", GaussianNB())])

# define the parameter grid for SelectKBest, 
# using the name from the pipeline followed by 2 underscores:
parameters = {'SKB__k': range(1,14)}

# Use the pipeline in GridSearchCV, with the parameter 'grid'
# using 'f1' as the scoring metric (as it is the weighted average
# of precision and recall):
gs = GridSearchCV(pipe, param_grid = parameters, scoring = 'f1')

# fit GridSearchCV:
gs.fit(X_train, Y_train)

# extract the best algorithm:
clf = gs.best_estimator_


#Instead of using a standard test/train split, you can use 'StratifiedShuffleSplit'
# create an instance of 'StratifiedShuffleSplit', 
# in this case '100' refers to the number of folds
# that is, the number of test/train splits
#from sklearn import StratifiedShuffleSplit
from sklearn.cross_validation import StratifiedShuffleSplit
sk_fold = StratifiedShuffleSplit(Y, 100, random_state = 42)

# use this cross validation method in GridSearchCV:
gs = GridSearchCV(pipe, param_grid = parameters, cv=sk_fold, scoring = 'f1')

# with 'StratifiedShuffleSplit' you fit the complete dataset
# GridSearchCV, internally, will use the indices from 'StratifiedShuffleSplit'
# to fit all 100 folds (all 100 test/train subsets).
gs.fit(X,Y)

# extract the best algorithm:
clf = gs.best_estimator_

print 'best algorithm using strat_s_split'
print clf

print 'best algorithm'
print clf

print "```````````````````````"























print "PIPELINE method"
# create feature union
features = []
# Feature Extraction with Principal Component Analysis (3 features)
features.append(('pca', PCA(n_components=3)))
#Feature Extraction with Statistical Selection (6 features)
features.append(('select_best', SelectKBest(k=6)))
# Feature Union
feature_union = FeatureUnion(features)
# create pipeline
#Python scikit-learn provides a Pipeline utility to help automate machine learning workflows.
#Pipelines work by allowing for a linear sequence of data transforms to be chained together culminating in a modeling process that can be evaluated.
#Pipelines help you prevent data leakage in your test harness by ensuring that data preparation like standardization is constrained to each fold of your cross validation procedure.

estimators = []
estimators.append(('feature_union', feature_union))
#estimators.append(('logistic', LogisticRegression()))
estimators.append(('KNN', KNeighborsClassifier()))
clf = Pipeline(estimators)

#print model

# evaluate pipeline
# The pipeline is then evaluated using 10-fold cross validation.
num_folds = 10
num_instances = len(X)
seed = 7
kfold = KFold(n=num_instances, n_folds=num_folds, random_state=seed)
score = cross_val_score(clf, X, Y, cv=kfold)
print 'cross_val_score=', (score.mean())

print "`````````````"


















"""
import numpy as np
from sklearn.linear_model import Ridge
from sklearn.grid_search import GridSearchCV

# prepare a range of alpha values to test
alphas = np.array([1,0.1,0.01,0.001,0.0001,0])
# create and fit a ridge regression model, testing each alpha
model = Ridge()
grid = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))
grid.fit(X,Y)
print(grid)
# summarize the results of the grid search
print(grid.best_score_)
print(grid.best_estimator_.alpha)
"""
